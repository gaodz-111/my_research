import os
os.environ["CUDA_VISIBLE_DEVICES"] = "6"
import torch
from modelscope import StableDiffusionPipeline
from PIL import Image
import torch.nn as nn
import torch.nn.functional as F
from diffusers import AutoencoderKL
import sys
from tqdm import tqdm
import json
# å†…å­˜ä¼˜åŒ–ä¾èµ–ï¼ˆå¯é€‰ï¼‰
try:
    import xformers
    XFORMERS_AVAILABLE = True
except ImportError:
    XFORMERS_AVAILABLE = False

# ===== LLaVA ä¾èµ– =====
parent_dir = os.path.abspath("./llava")  # ç¡®ä¿æŒ‡å‘æ­£ç¡®çš„llavaç›®å½•
sys.path.append(parent_dir)
from conversation import conv_templates
from llava.model.builder import load_pretrained_model
from llava.mm_utils import tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX
from get_codebook import VQDiffusionVAE  # ä½ çš„VQDiffusionVAEå®šä¹‰
from train_Hypernetwork import HypernetworkForCodebook, LLaVAStyleEncoder, vq_quantize  # ç¡®ä¿vq_quantizeå·²å®šä¹‰


# 1. å…¨å±€é…ç½®ï¼ˆæ ¹æ®ä½ çš„è·¯å¾„ä¿®æ”¹ï¼‰
MODEL_ID = "/data2/gaodz/stable-diffusion-2-1-base"  # SDæ¨¡å‹è·¯å¾„
TRAINED_PTH = "/data2/gaodz/VQDiffusionVAE/epoch_10.pth"  # è®­ç»ƒå¥½çš„VAE+Codebookè·¯å¾„
HY_PATH = "/data2/gaodz/HypernetworkVQ/hypernet_epoch_10.pth"  # Hypernetworkè·¯å¾„
LLAVA_MODEL_PATH = "/data2/gaodz/llava-v1.6-vicuna-7b"  # LLaVAæ¨¡å‹è·¯å¾„
JSON_PATH = "/data2/gaodz/Re-Align/hypernet_train_data.json"  # æµ‹è¯•æ•°æ®JSON
IMAGE_ROOT = "/data2/gaodz/OmniConsistency"  # å‚è€ƒå›¾ç‰‡æ ¹ç›®å½•ï¼ˆä¸JSONä¸­imageè·¯å¾„æ‹¼æ¥ï¼‰
OUTPUT_DIR_SD = "/data2/gaodz/VQ_test/SD"  # åŸå§‹æ¨¡å‹è¾“å‡ºç›®å½•
OUTPUT_DIR_HY = "/data2/gaodz/VQ_test/HY"  # è°ƒåˆ¶æ¨¡å‹è¾“å‡ºç›®å½•
os.makedirs(OUTPUT_DIR_SD, exist_ok=True)
os.makedirs(OUTPUT_DIR_HY, exist_ok=True)
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.float16 if device == "cuda" else torch.float32  # ç»Ÿä¸€æ•°æ®ç±»å‹


# 2. åŠ è½½åŸå§‹SDæ¨¡å‹ï¼ˆæ— è°ƒåˆ¶ï¼Œç”¨äºå¯¹æ¯”ï¼‰
def load_original_model(model_path):
    pipe = StableDiffusionPipeline.from_pretrained(
        model_path,
        torch_dtype=dtype,
        low_cpu_mem_usage=True  # å†…å­˜ä¼˜åŒ–
    ).to(device)
    # é«˜æ•ˆæ³¨æ„åŠ›ï¼ˆå¯é€‰ï¼Œéœ€å®‰è£…xformersï¼‰
    if XFORMERS_AVAILABLE and device == "cuda":
        pipe.enable_xformers_memory_efficient_attention()
    return pipe


# 3. æ ¸å¿ƒï¼šåŠ è½½å¸¦Hypernetworkçš„æ¨¡å‹ï¼ˆä¿®å¤LLaVAå¤šæ¨¡æ€è¾“å…¥ï¼‰
def load_trained_hypernetwork(model_path, trained_vae_pth, trained_hypernet_pth, llava_model_path):
    # 3.1 åŠ è½½SDç®¡é“
    pipe = StableDiffusionPipeline.from_pretrained(
        model_path,
        torch_dtype=dtype,
        low_cpu_mem_usage=True
    ).to(device)
    if XFORMERS_AVAILABLE and device == "cuda":
        pipe.enable_xformers_memory_efficient_attention()

    # 3.2 åŠ è½½VAEå’ŒåŸºç¡€Codebook
    your_model = VQDiffusionVAE(
        model_path,
        codebook_K=8192,
        beta=0.25
    ).to(device, dtype=dtype)
    vae_checkpoint = torch.load(trained_vae_pth, map_location=device)
    # åŠ è½½VAEå’ŒCodebookæƒé‡ï¼ˆstrict=Falseå…¼å®¹ç»“æ„å·®å¼‚ï¼‰
    your_model.load_state_dict({
        **vae_checkpoint["vae_state_dict"],
        **vae_checkpoint["codebook_state_dict"]
    }, strict=False)
    your_model.eval()
    pipe.vae = your_model.diffusion_vae.to(dtype)  # æ›¿æ¢SDçš„VAE

    # 3.3 åŠ è½½Hypernetworkï¼ˆè°ƒåˆ¶å±‚ï¼‰
    hypernet_checkpoint = torch.load(trained_hypernet_pth, map_location=device)
    hypernet = HypernetworkForCodebook(
        codebook_K=hypernet_checkpoint["codebook_K"],
        codebook_D=hypernet_checkpoint["codebook_D"],
        style_dim=4096,  # LLaVA-7Bè¾“å‡ºç»´åº¦
        modulation_type="affine"
    ).to(device, dtype=dtype)
    hypernet.load_state_dict(hypernet_checkpoint["hypernet_state_dict"], strict=True)
    hypernet.eval()
    print("âœ… æˆåŠŸåŠ è½½Hypernetwork")

    # 3.4 åŠ è½½LLaVAï¼ˆç”¨äºæå–â€œå›¾ç‰‡+æ–‡æœ¬â€å¤šæ¨¡æ€å‘é‡ï¼‰
    tokenizer, llava_model, image_processor, _ = load_pretrained_model(
        llava_model_path,
        None,
        "llava_v1.6",
        device=device,
        torch_dtype=dtype  # ä¸SDç»Ÿä¸€ç²¾åº¦
    )
    llava_encoder = LLaVAStyleEncoder(model=llava_model, image_processor=image_processor)
    print("âœ… æˆåŠŸåŠ è½½LLaVAé£æ ¼ç¼–ç å™¨")

    # 3.5 æ ¸å¿ƒä¿®å¤ï¼šä¿®è¡¥VAEçš„encodeé€»è¾‘ï¼ˆæ¥æ”¶å‚è€ƒå›¾ç‰‡+æ–‡æœ¬ï¼‰
    def patched_encode(self, x, reference_image=None, reference_text=None):
        """
        ä¿®å¤åï¼šLLaVAç”¨â€œå‚è€ƒå›¾ç‰‡+å‚è€ƒæ–‡æœ¬â€æå‘é‡ï¼Œè°ƒåˆ¶Codebook
        reference_image: å¤–éƒ¨ä¼ å…¥çš„å‚è€ƒå›¾ç‰‡å¼ é‡ï¼ˆ[1,3,224,224]ï¼‰
        reference_text: å‚è€ƒå›¾ç‰‡å¯¹åº”çš„æ–‡æœ¬æè¿°ï¼ˆå«é£æ ¼ä¿¡æ¯ï¼‰
        """
        with torch.no_grad():
            # æ­¥éª¤1ï¼šç”ŸæˆVAEæ½œå˜é‡z_eï¼ˆSDç”Ÿæˆæµç¨‹ä¸å˜ï¼‰
            z_e = your_model.diffusion_vae.encode(x).latent_dist.sample()
            z_e = z_e * your_model.scale_factor  # åº”ç”¨SD VAEçš„ç¼©æ”¾å› å­

            # æ­¥éª¤2ï¼šLLaVAæå–â€œå‚è€ƒå›¾ç‰‡+å‚è€ƒæ–‡æœ¬â€çš„å¤šæ¨¡æ€å‘é‡ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
            if reference_image is None or reference_text is None:
                # æ— å‚è€ƒæ—¶ç”¨é»˜è®¤é£æ ¼ï¼ˆå…¼å®¹é€»è¾‘ï¼Œå®é™…æµ‹è¯•ä¸ä¼šèµ°è¿™é‡Œï¼‰
                style_prompt = "default style"
                llava_prompt = f"<image>{style_prompt}"
                # ç”¨xå ä½ï¼ˆæ— å‚è€ƒæ—¶çš„ fallbackï¼‰
                processed_img = image_processor(x, return_tensors="pt")["pixel_values"].to(device, dtype=dtype)
            else:
                # æœ‰å‚è€ƒæ—¶ï¼šç”¨â€œå‚è€ƒæ–‡æœ¬+å‚è€ƒå›¾ç‰‡â€ï¼ˆä½ çš„æ ¸å¿ƒéœ€æ±‚ï¼‰
                # LLaVAè¦æ±‚promptå¿…é¡»åŒ…å«<image>æ ‡è®°ï¼ˆåŒ¹é…è®­ç»ƒæ ¼å¼ï¼‰
                llava_prompt = f"<image>{reference_text}"
                # é¢„å¤„ç†å‚è€ƒå›¾ç‰‡ï¼ˆåŒ¹é…LLaVAè¾“å…¥æ ¼å¼ï¼š224x224ï¼Œå½’ä¸€åŒ–ï¼‰
                processed_img = image_processor(
                    reference_image,
                    return_tensors="pt"
                )["pixel_values"].to(device, dtype=dtype)  # [1,3,224,224]

            # æ„é€ LLaVAçš„å¤šæ¨¡æ€è¾“å…¥batch
            llava_batch = {
                "input_ids": tokenizer(llava_prompt, return_tensors="pt").input_ids.to(device),
                "attention_mask": torch.ones_like(tokenizer(llava_prompt, return_tensors="pt").input_ids).to(device),
                "image": processed_img  # ä¼ å…¥å‚è€ƒå›¾ç‰‡ï¼ˆä¸æ˜¯xï¼ï¼‰
            }
            # æå–å¤šæ¨¡æ€é£æ ¼å‘é‡ï¼ˆå›¾ç‰‡+æ–‡æœ¬èåˆï¼‰
            style_emb = llava_encoder.get_style_embedding(llava_batch).to(dtype)  # [1,4096]

            # æ­¥éª¤3ï¼šHypernetworkè°ƒåˆ¶Codebook
            modulated_codebook = hypernet(style_emb, your_model.codebook.weight)  # [1,8192,4]

            # æ­¥éª¤4ï¼šç”¨è°ƒåˆ¶åçš„Codebooké‡åŒ–z_e
            z_q_st_list = []
            for b in range(x.shape[0]):
                z_e_b = z_e[b:b + 1]  # å•ä¸ªæ ·æœ¬çš„æ½œå˜é‡ [1,4,32,32]
                # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºè°ƒåˆ¶åçš„Codebook Embedding
                codebook_b = nn.Embedding.from_pretrained(modulated_codebook[b]).to(device, dtype=dtype)
                z_q_st_b, _, _ = vq_quantize(z_e_b, codebook_b)  # å¤ç”¨è®­ç»ƒæ—¶çš„é‡åŒ–å‡½æ•°
                z_q_st_list.append(z_q_st_b)
            z_q = torch.cat(z_q_st_list, dim=0)  # [B,4,32,32]

            return z_q.to(dtype)

    # æ›¿æ¢VAEçš„encodeæ–¹æ³•ï¼ˆè®©SDç”Ÿæˆæ—¶è‡ªåŠ¨è°ƒç”¨è°ƒåˆ¶é€»è¾‘ï¼‰
    pipe.vae.encode = patched_encode.__get__(pipe.vae)
    print("âœ… å·²ä¿®å¤VAEç¼–ç é€»è¾‘ï¼Œæ”¯æŒâ€˜å›¾ç‰‡+æ–‡æœ¬â€™è°ƒåˆ¶")

    # è¿”å›ç®¡é“å’Œä¾èµ–ç»„ä»¶ï¼ˆllava_encoderç”¨äºé¢„å¤„ç†å‚è€ƒå›¾ï¼‰
    return pipe, hypernet, llava_encoder, image_processor, tokenizer


# 4. æ ¸å¿ƒï¼šç”Ÿæˆå¯¹æ¯”å‡½æ•°ï¼ˆæ”¯æŒä¼ å…¥å‚è€ƒå›¾ç‰‡+æ–‡æœ¬ï¼‰
def generate_comparison(
    original_pipe, trained_pipe,
    gen_prompt, prompt_name,
    reference_image, reference_text,
    image_processor  # ç”¨äºé¢„å¤„ç†å‚è€ƒå›¾ç‰‡
):
    """
    ç”Ÿæˆå¯¹æ¯”å›¾ï¼šåŸå§‹SD vs è°ƒåˆ¶SD
    gen_prompt: è¦ç”Ÿæˆçš„å†…å®¹æ–‡æœ¬ï¼ˆå¦‚â€œä¸€åªçŒ«â€ï¼‰
    prompt_name: è¾“å‡ºæ–‡ä»¶åå‰ç¼€
    reference_image: PILæ ¼å¼çš„å‚è€ƒå›¾ç‰‡ï¼ˆç”¨äºè°ƒåˆ¶ï¼‰
    reference_text: å‚è€ƒå›¾ç‰‡å¯¹åº”çš„æ–‡æœ¬ï¼ˆå«é£æ ¼ä¿¡æ¯ï¼‰
    image_processor: LLaVAçš„å›¾åƒå¤„ç†å™¨ï¼ˆé¢„å¤„ç†å‚è€ƒå›¾ï¼‰
    """
    gen_kwargs = {
        "num_inference_steps": 50,
        "guidance_scale": 7.5,
        "width": 512,
        "height": 512,
        "return_dict": True
    }

    # --------------------------
    # 1. åŸå§‹æ¨¡å‹ç”Ÿæˆï¼ˆæ— è°ƒåˆ¶ï¼‰
    # --------------------------
    with torch.inference_mode():
        original_out = original_pipe(gen_prompt, **gen_kwargs)
        original_image = original_out.images[0]
    # ä¿å­˜åŸå§‹æ¨¡å‹ç»“æœ
    original_save_path = os.path.join(OUTPUT_DIR_SD, f"{prompt_name}.png")
    original_image.save(original_save_path)
    print(f"ğŸ“Œ åŸå§‹æ¨¡å‹ç»“æœå·²ä¿å­˜ï¼š{original_save_path}")

    # --------------------------
    # 2. è°ƒåˆ¶æ¨¡å‹ç”Ÿæˆï¼ˆç”¨å‚è€ƒå›¾ç‰‡+æ–‡æœ¬ï¼‰
    # --------------------------
    with torch.inference_mode():
        # é¢„å¤„ç†å‚è€ƒå›¾ç‰‡ï¼ˆè½¬ä¸ºå¼ é‡ï¼ŒåŒ¹é…LLaVAè¾“å…¥ï¼‰
        processed_ref_img = image_processor(
            reference_image,
            return_tensors="pt"
        )["pixel_values"].to(device, dtype=dtype)  # [1,3,224,224]

        # å…³é”®ï¼šè®©trained_pipeçš„VAEèƒ½æ‹¿åˆ°å‚è€ƒå›¾ç‰‡å’Œæ–‡æœ¬
        trained_pipe.vae.reference_image = processed_ref_img
        trained_pipe.vae.reference_text = reference_text

        # ç”Ÿæˆï¼ˆSDç”Ÿæˆæµç¨‹ä¸å˜ï¼Œä½†VAE encodeæ—¶ä¼šè‡ªåŠ¨è°ƒåˆ¶ï¼‰
        trained_out = trained_pipe(gen_prompt, **gen_kwargs)
        trained_image = trained_out.images[0]

        # æ¸…ç†ä¸´æ—¶å˜é‡ï¼ˆé¿å…å†…å­˜å †ç§¯ï¼‰
        del trained_pipe.vae.reference_image
        del trained_pipe.vae.reference_text

    # ä¿å­˜è°ƒåˆ¶æ¨¡å‹ç»“æœ
    trained_save_path = os.path.join(OUTPUT_DIR_HY, f"{prompt_name}.png")
    trained_image.save(trained_save_path)
    print(f"ğŸ“Œ è°ƒåˆ¶æ¨¡å‹ç»“æœå·²ä¿å­˜ï¼š{trained_save_path}")

    # æ¸…ç†å†…å­˜
    del original_image, trained_image, original_out, trained_out
    torch.cuda.empty_cache()

    return original_save_path, trained_save_path


# 5. ä¸»æµ‹è¯•é€»è¾‘ï¼ˆåŠ è½½æ•°æ®â†’ç”Ÿæˆå¯¹æ¯”ï¼‰
if __name__ == "__main__":
    # 5.1 åŠ è½½æ¨¡å‹
    print("ğŸ”„ åŠ è½½åŸå§‹SDæ¨¡å‹...")
    original_pipe = load_original_model(MODEL_ID)

    print("\nğŸ”„ åŠ è½½å¸¦Hypernetworkçš„è°ƒåˆ¶æ¨¡å‹...")
    trained_pipe, hypernet, llava_encoder, image_processor, tokenizer = load_trained_hypernetwork(
        MODEL_ID, TRAINED_PTH, HY_PATH, LLAVA_MODEL_PATH
    )

    # 5.2 åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆé€è¡Œè¯»å–ï¼Œé¿å…å†…å­˜å †ç§¯ï¼‰
    print(f"\nğŸ“„ åŠ è½½æµ‹è¯•æ•°æ®ï¼š{JSON_PATH}")
    with open(JSON_PATH, "r", encoding="utf-8") as f:
        # ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        index = 0
        for line_num, line in enumerate(tqdm(f, desc="å¤„ç†æµ‹è¯•æ•°æ®")):
            try:
                item = json.loads(line)

                # 5.2.1 æå–å…³é”®ä¿¡æ¯ï¼ˆä»JSON itemä¸­ï¼‰
                # å‚è€ƒå›¾ç‰‡è·¯å¾„ï¼ˆæ‹¼æ¥æ ¹ç›®å½•ï¼‰
                ref_img_filename = item["image"]  # JSONä¸­å­˜å‚¨çš„å›¾ç‰‡æ–‡ä»¶åï¼ˆå¦‚"xxx.png"ï¼‰
                ref_img_path = os.path.join(IMAGE_ROOT, ref_img_filename)
                # å‚è€ƒæ–‡æœ¬ï¼ˆç”¨æˆ·é—®é¢˜+æ¨¡å‹å›ç­”ï¼Œå«é£æ ¼ä¿¡æ¯ï¼‰
                qs = item["conversations"][0]["value"].replace("<image>\n", "")  # å»é™¤åŸå§‹<image>æ ‡è®°
                answer = item["conversations"][1]["value"]
                system_prompt = "You are a VQA assistant and you need to describe the content of the image in different styles of text."
                reference_text = system_prompt + "\n" + f"USER: {qs}\nASSISTANT: {answer}"
                # ç”Ÿæˆç›®æ ‡æ–‡æœ¬ï¼ˆè¦ç”Ÿæˆçš„å†…å®¹ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œè¿™é‡Œç”¨ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒå†…å®¹ï¼‰
                gen_prompt = qs  # ä¾‹å¦‚ï¼šè‹¥qsæ˜¯â€œæè¿°è¿™å¼ å›¾çš„é£æ ¼â€ï¼Œå¯æ”¹ä¸ºå…·ä½“ç”Ÿæˆå†…å®¹å¦‚â€œä¸€åªç‹—â€
                # è¾“å‡ºæ–‡ä»¶åï¼ˆç”¨å›¾ç‰‡å+è¡Œå·ï¼Œé¿å…é‡å¤ï¼‰
                prompt_name = index

                # 5.2.2 åŠ è½½å¹¶é¢„å¤„ç†å‚è€ƒå›¾ç‰‡ï¼ˆPILâ†’ä¿æŒåŸå§‹å°ºå¯¸ï¼Œåç»­ç”±LLaVAå¤„ç†å™¨ç¼©æ”¾ï¼‰
                if not os.path.exists(ref_img_path):
                    print(f"âš ï¸ å‚è€ƒå›¾ç‰‡ä¸å­˜åœ¨ï¼š{ref_img_path}ï¼Œè·³è¿‡è¯¥æ ·æœ¬")
                    continue
                reference_image = Image.open(ref_img_path).convert("RGB")  # åŠ è½½ä¸ºRGB

                # 5.2.3 ç”Ÿæˆå¯¹æ¯”å›¾ï¼ˆæ ¸å¿ƒè°ƒç”¨ï¼‰
                print(f"\nğŸ¯ å¤„ç†æ ·æœ¬ {line_num+1}ï¼š{ref_img_filename}")
                generate_comparison(
                    original_pipe=original_pipe,
                    trained_pipe=trained_pipe,
                    gen_prompt=gen_prompt,
                    prompt_name=prompt_name,
                    reference_image=reference_image,
                    reference_text=reference_text,
                    image_processor=image_processor
                )
                index += 1
            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬{line_num+1}è¡Œæ ·æœ¬å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡")
                continue

    # 5.3 æµ‹è¯•å®Œæˆï¼Œé‡Šæ”¾èµ„æº
    print("\nâœ… æ‰€æœ‰æ ·æœ¬å¤„ç†å®Œæˆï¼")
    del original_pipe, trained_pipe, hypernet, llava_encoder, image_processor, tokenizer
    torch.cuda.empty_cache()
